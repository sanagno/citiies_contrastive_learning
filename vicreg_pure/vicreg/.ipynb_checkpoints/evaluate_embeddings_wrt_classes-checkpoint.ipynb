{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34444cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import math\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb4d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import resnet\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import math\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True   #OTHERWISE TRUNCATED IMAGE FILE ERROR SOMEWHERE IN ENUMERATE(DATALOADER)!!!!\n",
    "\n",
    "import resnet\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from munkres import Munkres\n",
    "\n",
    "import augmentations as aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4290e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077e6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SN7Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, transform=None, train=True):\n",
    "        \n",
    "        if train == True:\n",
    "            self.img_ids_labels = pd.read_csv(\"../../ids_and_labels_train_repeated.csv\", header = None) \n",
    "        else:\n",
    "            self.img_ids_labels = pd.read_csv(\"../../ids_and_labels_val_repeated.csv\", header = None)\n",
    "            \n",
    "        self.img_dir = '/local/home/stuff/datasets/Challenge_7/train'\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.img_ids_labels.iloc[idx, 0])\n",
    "        image_id, patch_id = self.img_ids_labels.iloc[idx, 0].split(\"!\")[1],self.img_ids_labels.iloc[idx, 0].split(\"!\")[0]\n",
    "        pattern = \"mosaic_(.*?).tif\"\n",
    "        location_id = re.search(pattern, image_id).group(1)\n",
    "        #print(image_id)\n",
    "        #print(location_id)\n",
    "        img_path = os.path.join(self.img_dir, location_id, 'images', image_id )\n",
    "        image = torchvision.transforms.ToTensor()(Image.open(img_path))[0:3,:,:]  #TAKE RGB CHANNELS ONLY FOR RESNET COMPATIBILITY!!!\n",
    "        \n",
    "        image_padded = torch.nn.functional.pad(image, pad=(0, 1024 - image.shape[2], 0, 1024 - image.shape[1]))\n",
    "        patches = image_padded.unfold(1, 256, 256).unfold(2, 256, 256)\n",
    "        patches = patches.reshape(3, -1, 256, 256)\n",
    "        patches = patches.permute(1,0,2,3)\n",
    "        \n",
    "        image = patches[int(patch_id)]\n",
    "        \n",
    "        label = torch.from_numpy(np.asarray(self.img_ids_labels.iloc[idx, 3])).type(torch.LongTensor)\n",
    "        \n",
    "        #patches = patches.view(-1, 3, 256, 256)\n",
    "        #print(patches.shape)\n",
    "        #label = label.flatten()\n",
    "        #label = label.type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "        image = self.transform(image)\n",
    "                \n",
    "            \n",
    "        #if self.target_transform:\n",
    "            #label = self.target_transform(label)\n",
    "            \n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.img_ids_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee23f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([ transforms.Resize(224), torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "seen_transforms = transforms.Compose(\n",
    "            [   \n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                #transforms.Resize(224),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "#seen_transforms = aug.TrainTransform()\n",
    "\n",
    "unseen_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.CenterCrop(224),\n",
    "                #transforms.Resize(224),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "#unseen_transforms = seen_transforms\n",
    "                \n",
    "test_seen_dataset = SN7Dataset(train=True, transform = seen_transforms)\n",
    "test_unseen_dataset = SN7Dataset(train=False, transform = unseen_transforms)\n",
    "\n",
    "test_seen_loader = DataLoader(dataset=test_seen_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_unseen_loader = DataLoader(dataset=test_unseen_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33ba3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12268863488, 12806062080)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.mem_get_info(torch.cuda.current_device()))\n",
    "device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5226e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST TRY LOADING MODEL LIKE THIS\n",
    "#backbone, embedding = resnet.__dict__['resnet18'](zero_init_residual=True)\n",
    "#backbone.load_state_dict(torch.load('exp/resnet18_2.pth'))\n",
    "#backbone.to(device)\n",
    "#backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fba5a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone, embedding = resnet.__dict__['resnet18'](zero_init_residual=True)\n",
    "state_dict = torch.load('exp/resnet18_new_4096_200.pth', map_location=\"cpu\")\n",
    "if \"model\" in state_dict:\n",
    "    state_dict = state_dict[\"model\"]\n",
    "    state_dict = {key.replace(\"module.backbone.\", \"\"): value for (key, value) in state_dict.items()}\n",
    "backbone.load_state_dict(state_dict, strict=False)\n",
    "backbone.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7052870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "labels_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_seen_loader):\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        #print(inputs[1].shape)\n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_list.append(embedding)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d799060",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_seen_arr = np.zeros((len(test_seen_loader), 512))\n",
    "counter = 0\n",
    "for embedding in embeddings_list:\n",
    "    embeddings_seen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "    counter += 1\n",
    "#embeddings_unseen_arr = torch.tensor(embeddings_unseen_list)\n",
    "#labels_unseen_arr = np.array(labels_unseen_list)\n",
    "\n",
    "print(embeddings_seen_arr)\n",
    "labels_seen_arr = np.zeros(len(test_seen_loader))\n",
    "counter = 0\n",
    "for i in labels_list:\n",
    "    labels_seen_arr[counter] = i.detach().numpy()\n",
    "    counter += 1\n",
    "print(labels_seen_arr)\n",
    "\n",
    "#np.savetxt(\"seen_embeddings.csv\", embeddings_seen_arr, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unseen_list = []\n",
    "embeddings_unseen_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_unseen_loader):\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_unseen_list.append(embedding)\n",
    "        labels_unseen_list.append(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_unseen_arr = np.zeros((len(test_unseen_loader), 512))\n",
    "counter = 0\n",
    "for embedding in embeddings_unseen_list:\n",
    "    embeddings_unseen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "    counter += 1\n",
    "#embeddings_unseen_arr = torch.tensor(embeddings_unseen_list)\n",
    "#labels_unseen_arr = np.array(labels_unseen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_unseen_arr)\n",
    "labels_unseen_arr = np.zeros(len(test_unseen_loader))\n",
    "counter = 0\n",
    "for i in labels_unseen_list:\n",
    "    labels_unseen_arr[counter] = i.detach().numpy()\n",
    "    counter += 1\n",
    "print(labels_unseen_arr)\n",
    "\n",
    "#np.savetxt(\"unseen_embeddings.csv\", embeddings_unseen_arr, delimiter=\",\", fmt='%s')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(embeddings_seen_arr.shape[0]):\n",
    "    if len(np.unique(embeddings_seen_arr[i,:]))==1:\n",
    "        print('yes')\n",
    "for i in range(embeddings_unseen_arr.shape[0]):\n",
    "    if len(np.unique(embeddings_unseen_arr[i,:]))==1:\n",
    "        print('yes_unseen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST TRY DELETE LATER\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embeddings_unseen_arr_trial = embeddings_unseen_arr[0:120,:]\n",
    "labels_unseen_arr_trial = labels_unseen_arr[0:120]\n",
    "\n",
    "#pca = PCA(n_components=2)\n",
    "#pca.fit(embeddings_unseen_arr)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "\n",
    "#embeddings_reduced = pca.transform(embeddings_unseen_arr)\n",
    "#print(embeddings_reduced)\n",
    "\n",
    "u_labels = np.unique(labels_unseen_arr_trial)\n",
    "print(u_labels)\n",
    "\n",
    "embeddings_reduced = TSNE(n_components=2, learning_rate='auto',init='random', perplexity = 10.0).fit_transform(embeddings_unseen_arr_trial)\n",
    "\n",
    "for i in u_labels:\n",
    "    \n",
    "    plt.scatter(embeddings_reduced[labels_unseen_arr_trial == i , 0] , embeddings_reduced[labels_unseen_arr_trial == i , 1] , label = i)\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neighbours = [1, 3, 5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for k in n_neighbours:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(embeddings_seen_arr, labels_seen_arr)\n",
    "    labels_predicted = knn.predict(embeddings_unseen_arr)\n",
    "\n",
    "    #print('Accuracy' + str(k) + ':', np.sum(labels_unseen_arr == labels_predicted)/len(labels_unseen_arr))\n",
    "    print('Accuracy:', knn.score(embeddings_unseen_arr, labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4314106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_preds(cluster_assignments, y_true, n_clusters):\n",
    "    '''\n",
    "    Computes the predicted labels, where label assignments now\n",
    "    correspond to the actual labels in y_true (as estimated by Munkres)\n",
    "\n",
    "    cluster_assignments:    array of labels, outputted by kmeans\n",
    "    y_true:                 true labels\n",
    "    n_clusters:             number of clusters in the dataset\n",
    "\n",
    "    returns:    a tuple containing the accuracy and confusion matrix,\n",
    "                in that order\n",
    "    '''\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_true, cluster_assignments, labels=None)\n",
    "    # compute accuracy based on optimal 1:1 assignment of clusters to labels\n",
    "    cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters)\n",
    "    indices = Munkres().compute(cost_matrix)\n",
    "    kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)\n",
    "    y_pred = kmeans_to_true_cluster_labels[cluster_assignments]\n",
    "    return y_pred, confusion_matrix \n",
    "\n",
    "def calculate_cost_matrix(C, n_clusters):\n",
    "    cost_matrix = np.zeros((n_clusters, n_clusters))\n",
    "\n",
    "    # cost_matrix[i,j] will be the cost of assigning cluster i to label j\n",
    "    for j in range(n_clusters):\n",
    "        s = np.sum(C[:,j]) # number of examples in cluster i\n",
    "        for i in range(n_clusters):\n",
    "            t = C[i,j]\n",
    "            cost_matrix[j,i] = s-t\n",
    "    return cost_matrix\n",
    "\n",
    "def get_cluster_labels_from_indices(indices):\n",
    "    n_clusters = len(indices)\n",
    "    clusterLabels = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        clusterLabels[i] = indices[i][1]\n",
    "    return clusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cd153ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Afteer Hungarian Assignment: 0.20692090395480225\n",
      "0.8763344876196063\n",
      "(5664,)\n",
      "Accuracy Afteer Hungarian Assignment: 0.19050141242937854\n",
      "0.8820398867263064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#labels_predicted = AgglomerativeClustering(n_clusters = 20).fit_predict(embeddings_unseen_arr)\n",
    "\n",
    "#truth = labels_unseen_arr\n",
    "#pred = labels_predicted\n",
    "#print('Accuracy Afteer Hungarian Assignment:',np.sum( get_y_preds(pred, truth, 20)[0] == truth )/len(truth) )\n",
    "\n",
    "#print(sklearn.metrics.rand_score(labels_unseen_arr, labels_predicted))\n",
    "\n",
    "whole_test_set = np.concatenate((embeddings_seen_arr,embeddings_unseen_arr), axis=0)\n",
    "whole_labels = np.concatenate((labels_seen_arr, labels_unseen_arr))\n",
    "\n",
    "labels_predicted = AgglomerativeClustering(n_clusters = 20).fit_predict(whole_test_set)\n",
    "\n",
    "truth = whole_labels\n",
    "pred = labels_predicted\n",
    "print('Accuracy Afteer Hungarian Assignment:',np.sum( get_y_preds(pred, truth, 20)[0] == truth )/len(truth) )\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#kmeans = KMeans(n_clusters=20, random_state=36).fit(embeddings_seen_arr)\n",
    "#labels_predicted = kmeans.predict(embeddings_unseen_arr)\n",
    "#print(labels_predicted.shape)\n",
    "\n",
    "#truth = labels_unseen_arr\n",
    "#pred = labels_predicted\n",
    "#print('Accuracy Afteer Hungarian Assignment:',np.sum( get_y_preds(pred, truth, 20)[0] == truth )/len(truth) )\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=36).fit(whole_test_set)\n",
    "labels_predicted = kmeans.predict(whole_test_set)\n",
    "print(whole_labels.shape)\n",
    "\n",
    "truth = whole_labels\n",
    "pred = labels_predicted\n",
    "print('Accuracy Afteer Hungarian Assignment:',np.sum( get_y_preds(pred, truth, 20)[0] == truth )/len(truth) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b70a10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "seen_transforms = transforms.Compose(\n",
    "            [   \n",
    "                #transforms.RandomResizedCrop(224),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.Resize(224),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "unseen_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.CenterCrop(224),\n",
    "                #transforms.Resize(224),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "test_seen_dataset = SN7Dataset(train=True, transform = seen_transforms)\n",
    "test_unseen_dataset = SN7Dataset(train=False, transform = unseen_transforms)\n",
    "\n",
    "test_seen_loader = DataLoader(dataset=test_seen_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_unseen_loader = DataLoader(dataset=test_unseen_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de8edc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####FOR SUPERVİSED MODEL!!!!\n",
    "\n",
    "backbone, embedding = resnet.__dict__['resnet18'](zero_init_residual=True)\n",
    "state_dict = torch.load('../../Supervised_Model.pth', map_location=\"cpu\")\n",
    "if \"model\" in state_dict:\n",
    "    state_dict = state_dict[\"model\"]\n",
    "    state_dict = {key.replace(\"module.backbone.\", \"\"): value for (key, value) in state_dict.items()}\n",
    "backbone.load_state_dict(state_dict, strict=False)\n",
    "backbone.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e55c550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "297bb401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n"
     ]
    }
   ],
   "source": [
    "labels_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_seen_loader):\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_list.append(embedding)\n",
    "        labels_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c94851cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12317052 1.43663383 0.28230196 ... 1.80706692 0.14865744 0.87916923]\n",
      " [0.13682488 0.93744624 0.33663651 ... 3.37823725 0.28292724 1.25425076]\n",
      " [0.04704413 0.9674179  0.08414672 ... 3.42556    0.2273407  1.15283716]\n",
      " ...\n",
      " [0.02487592 0.32121474 0.15861818 ... 1.61708641 0.18078965 0.82726234]\n",
      " [0.07081281 0.41750196 0.31754136 ... 1.43292952 0.10991853 0.5478828 ]\n",
      " [0.03780255 0.66814345 0.69805986 ... 1.66296041 0.34856492 0.35852769]]\n",
      "[17. 17. 17. ... 13. 13. 13.]\n"
     ]
    }
   ],
   "source": [
    "embeddings_seen_arr = np.zeros((len(test_seen_loader), 512))\n",
    "counter = 0\n",
    "for embedding in embeddings_list:\n",
    "    embeddings_seen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "    counter += 1\n",
    "#embeddings_unseen_arr = torch.tensor(embeddings_unseen_list)\n",
    "#labels_unseen_arr = np.array(labels_unseen_list)\n",
    "\n",
    "print(embeddings_seen_arr)\n",
    "labels_seen_arr = np.zeros(len(test_seen_loader))\n",
    "counter = 0\n",
    "for i in labels_list:\n",
    "    labels_seen_arr[counter] = i.detach().numpy()\n",
    "    counter += 1\n",
    "print(labels_seen_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83c131d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n"
     ]
    }
   ],
   "source": [
    "labels_unseen_list = []\n",
    "embeddings_unseen_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_unseen_loader):\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_unseen_list.append(embedding)\n",
    "        labels_unseen_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b3de7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04541048 0.09411345 0.13167404 ... 1.29203272 0.02067739 1.65651369]\n",
      " [0.12516052 0.08320393 0.23238413 ... 1.51393843 0.0180764  1.39519823]\n",
      " [0.12817952 0.15544313 0.15145196 ... 1.0467217  0.04106529 1.34857178]\n",
      " ...\n",
      " [0.09706418 1.06956339 0.48926908 ... 2.52267575 0.00343556 2.48549795]\n",
      " [0.43610546 1.66794229 0.87992054 ... 1.33218634 0.23914057 1.55275536]\n",
      " [0.50597411 1.05220401 0.52905434 ... 1.76076078 0.03208228 2.0385375 ]]\n",
      "[30. 30. 30. ... 17. 17. 17.]\n"
     ]
    }
   ],
   "source": [
    "embeddings_unseen_arr = np.zeros((len(test_unseen_loader), 512))\n",
    "counter = 0\n",
    "for embedding in embeddings_unseen_list:\n",
    "    embeddings_unseen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "    counter += 1\n",
    "#embeddings_unseen_arr = torch.tensor(embeddings_unseen_list)\n",
    "#labels_unseen_arr = np.array(labels_unseen_list)\n",
    "\n",
    "print(embeddings_unseen_arr)\n",
    "labels_unseen_arr = np.zeros(len(test_unseen_loader))\n",
    "counter = 0\n",
    "for i in labels_unseen_list:\n",
    "    labels_unseen_arr[counter] = i.detach().numpy()\n",
    "    counter += 1\n",
    "print(labels_unseen_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a74e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8320974576271186\n",
      "Accuracy: 0.8319209039548022\n",
      "Accuracy: 0.834569209039548\n",
      "Accuracy: 0.8315677966101694\n",
      "Accuracy: 0.8260946327683616\n",
      "Accuracy: 0.8206214689265536\n",
      "Accuracy: 0.8163841807909604\n",
      "Accuracy: 0.8080861581920904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neighbours = [1, 3, 5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for k in n_neighbours:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(embeddings_seen_arr, labels_seen_arr)\n",
    "    labels_predicted = knn.predict(embeddings_unseen_arr)\n",
    "\n",
    "    #print('Accuracy' + str(k) + ':', np.sum(labels_unseen_arr == labels_predicted)/len(labels_unseen_arr))\n",
    "    print('Accuracy:', knn.score(embeddings_unseen_arr, labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f01ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 24 24 ... 19  1  1]\n",
      "0.9128087990135192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=31, random_state=36).fit(embeddings_seen_arr)\n",
    "labels_predicted = kmeans.predict(embeddings_unseen_arr)\n",
    "print(labels_predicted)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.rand_score(labels_unseen_arr, labels_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c213cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Afteer Hungarian Assignment: 0.5238347457627118\n",
      "0.9149049958547455\n",
      "(5664,)\n",
      "Accuracy Afteer Hungarian Assignment: 0.4980579096045198\n",
      "0.9128087990135192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "labels_predicted = AgglomerativeClustering(n_clusters = 31).fit_predict(embeddings_unseen_arr)\n",
    "\n",
    "truth = labels_unseen_arr\n",
    "pred = labels_predicted\n",
    "print('Accuracy Afteer Hungarian Assignment:',np.sum( get_y_preds(pred, truth, 31)[0] == truth )/len(truth) )\n",
    "\n",
    "print(sklearn.metrics.rand_score(labels_unseen_arr, labels_predicted))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=31, random_state=36).fit(embeddings_seen_arr)\n",
    "labels_predicted = kmeans.predict(embeddings_unseen_arr)\n",
    "print(labels_predicted.shape)\n",
    "\n",
    "truth = labels_unseen_arr\n",
    "pred = labels_predicted\n",
    "print('Accuracy Afteer Hungarian Assignment:',np.sum( get_y_preds(pred, truth, 31)[0] == truth )/len(truth) )\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.rand_score(labels_unseen_arr, labels_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98d54fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17024, 512)\n",
      "(5664, 512)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_seen_arr.shape)\n",
    "print(embeddings_unseen_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382c4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca4d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import resnet\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import math\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True   #OTHERWISE TRUNCATED IMAGE FILE ERROR SOMEWHERE IN ENUMERATE(DATALOADER)!!!!\n",
    "\n",
    "import resnet\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data_dir = '../../ids_and_labels_train_repeated.csv'\n",
    "        self.train_percent = 5\n",
    "        self.pretrained = './exp/resnet18.pth'\n",
    "        self.exp_dir = 'exp'\n",
    "        self.print_freq = 100\n",
    "        self.arch = 'resnet18'\n",
    "        self.epochs = 20\n",
    "        self.batch_size = 16\n",
    "        self.lr_backbone = 0.03\n",
    "        self.lr_head = 0.08\n",
    "        self.weight_decay = 0.0\n",
    "        self.weights = 'finetune'\n",
    "        self.workers = 10\n",
    "        self.rank = 0\n",
    "        self.dist_url = f\"tcp://localhost:{random.randrange(49152, 65535)}\"\n",
    "        self.world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f0a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416c2e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['global_monthly_2018_09_mosaic_L15-0331E-1257N_1327_3160_13.tif' nan\n",
      "  'California' 18]\n",
      " ['global_monthly_2018_06_mosaic_L15-0331E-1257N_1327_3160_13.tif' nan\n",
      "  'California' 18]\n",
      " ['global_monthly_2018_11_mosaic_L15-0331E-1257N_1327_3160_13.tif' nan\n",
      "  'California' 18]\n",
      " ...\n",
      " ['global_monthly_2019_01_mosaic_L15-1848E-0793N_7394_5018_13.tif'\n",
      "  'Melbourne' 'Victoria' 1]\n",
      " ['global_monthly_2018_12_mosaic_L15-1848E-0793N_7394_5018_13.tif'\n",
      "  'Melbourne' 'Victoria' 1]\n",
      " ['global_monthly_2019_06_mosaic_L15-1848E-0793N_7394_5018_13.tif'\n",
      "  'Melbourne' 'Victoria' 1]]\n"
     ]
    }
   ],
   "source": [
    "image_id_and_labels = pd.read_csv('../../ids_and_labels.csv',delimiter=',',header=1)\n",
    "#print(image_id_and_labels.values)\n",
    "image_id_and_labels_arr = np.array(image_id_and_labels.values)\n",
    "#print(image_id_and_labels_arr)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "image_id_and_labels_encoded = image_id_and_labels_arr.copy()\n",
    "label_encoder.fit(image_id_and_labels_encoded[:,3])\n",
    "encoded_countries = label_encoder.transform(image_id_and_labels_encoded[:,3])\n",
    "#print(encoded_countries)\n",
    "image_id_and_labels_encoded[:,3] = encoded_countries\n",
    "print(image_id_and_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372e1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "stratSplit = StratifiedShuffleSplit(n_splits=1, test_size = args.train_percent*0.01, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in stratSplit.split(image_id_and_labels_encoded, np.zeros(len(image_id_and_labels_encoded))):\n",
    "    ids_labels_train, ids_labels_train_subset = image_id_and_labels_encoded[train_idx], image_id_and_labels_encoded[test_idx]\n",
    "\n",
    "print(len(ids_labels_train_subset))\n",
    "np.savetxt(\"ids_and_labels_train_subset.csv\", ids_labels_train_subset, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b756b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fd5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, args):\n",
    "\n",
    "    torch.cuda.set_device(gpu)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    backbone, embedding = resnet.__dict__[args.arch](zero_init_residual=True)\n",
    "    state_dict = torch.load(args.pretrained, map_location=\"cpu\")\n",
    "    if \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "        state_dict = {\n",
    "            key.replace(\"module.backbone.\", \"\"): value\n",
    "            for (key, value) in state_dict.items()\n",
    "        }\n",
    "    backbone.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    head = nn.Linear(embedding, 31)                            #CHANGE ACCORDING TO NUMBER OF CLASSES!!!!!\n",
    "    head.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    head.bias.data.zero_()\n",
    "    model = nn.Sequential(backbone, head)\n",
    "    model.cuda(gpu)\n",
    "\n",
    "    if args.weights == \"freeze\":\n",
    "        backbone.requires_grad_(False)\n",
    "        head.requires_grad_(True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "\n",
    "    param_groups = [dict(params=head.parameters(), lr=args.lr_head)]\n",
    "    \n",
    "    if args.weights == \"finetune\":\n",
    "        param_groups.append(dict(params=backbone.parameters(), lr=args.lr_backbone))\n",
    "        \n",
    "    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
    "\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_acc = argparse.Namespace(top1=0, top5=0)\n",
    "\n",
    "    # Data loading code\n",
    "    \n",
    "    class SN7Dataset(Dataset):\n",
    "    \n",
    "        def __init__(self, transform=None, train=True):\n",
    "        \n",
    "            if train == True:\n",
    "                self.img_ids_labels = pd.read_csv(\"ids_and_labels_train_subset.csv\", header = None) \n",
    "            else:\n",
    "                self.img_ids_labels = pd.read_csv(\"../../ids_and_labels_val.csv\", header = None)\n",
    "            \n",
    "            self.img_dir = '/local/home/stuff/datasets/Challenge_7/train'\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "        \n",
    "            image_id = self.img_ids_labels.iloc[idx, 0]\n",
    "            pattern = \"mosaic_(.*?).tif\"\n",
    "            location_id = re.search(pattern, image_id).group(1)\n",
    "            #print(image_id)\n",
    "            #print(location_id)\n",
    "            img_path = os.path.join(self.img_dir, location_id, 'images', image_id )\n",
    "            image = torchvision.transforms.ToTensor()(Image.open(img_path))[0:3,:,:]  #TAKE RGB CHANNELS ONLY FOR RESNET COMPATIBILITY!!!\n",
    "        \n",
    "            image_padded = torch.nn.functional.pad(image, pad=(0, 1024 - image.shape[2], 0, 1024 - image.shape[1]))\n",
    "            patches = image_padded.unfold(1, 256, 256).unfold(2, 256, 256)\n",
    "            patches = patches.reshape(3, -1, 256, 256)\n",
    "            patches = patches.permute(1,0,2,3)\n",
    "        \n",
    "            label = torch.from_numpy(np.asarray(self.img_ids_labels.iloc[idx, 3]))*torch.ones(patches.size(0))\n",
    "        \n",
    "            #patches = patches.view(-1, 3, 256, 256)\n",
    "            #label = label.flatten()\n",
    "            #label = label.type(torch.LongTensor)\n",
    "        \n",
    "            \n",
    "            patches = self.transform(patches)\n",
    "            \n",
    "            return patches, label\n",
    "        \n",
    "        def __len__(self):\n",
    "        \n",
    "            return len(self.img_ids_labels)\n",
    "    \n",
    "    \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "   \n",
    "    train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                #transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    val_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                #transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    #DATASSET AND LOADERS\n",
    "    \n",
    "    train_dataset = SN7Dataset(train=True, transform = train_transforms)\n",
    "    val_dataset = SN7Dataset(train=False, transform = val_transforms)\n",
    "    \n",
    "    kwargs = dict(\n",
    "        batch_size=args.batch_size // args.world_size,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True, shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, **kwargs\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #WEIGHT FINETUNING/TRAINIG\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    checkpoints = 30\n",
    "    n_total_steps = len(train_loader)\n",
    "    \n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        # TRAIN\n",
    "        \n",
    "        if args.weights == \"finetune\":\n",
    "            model.train()\n",
    "        elif args.weights == \"freeze\":\n",
    "            model.eval()\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        for step, (images, target) in enumerate(\n",
    "            train_loader, start=epoch * len(train_loader)\n",
    "        ):\n",
    "            images = images.view(-1, 3, 224, 224)    \n",
    "            target = target.flatten()\n",
    "            target = target.type(torch.LongTensor)\n",
    "            \n",
    "            output = model(images.cuda(gpu, non_blocking=True))\n",
    "            loss = criterion(output, target.cuda(gpu, non_blocking=True))\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (step+1) % checkpoints == 0:\n",
    "            \n",
    "                print (f'Epoch [{epoch+1}/{args.epochs}], Step [{step+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "                print('training_loss', running_loss/checkpoints, epoch * n_total_steps + step)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # EVALUATE\n",
    "        \n",
    "        model.eval()\n",
    "        if args.rank == 0:\n",
    "            top1 = AverageMeter(\"Acc@1\")\n",
    "            top5 = AverageMeter(\"Acc@5\")\n",
    "            with torch.no_grad():\n",
    "                for images, target in val_loader:\n",
    "                    \n",
    "                    images = images.view(-1, 3, 224, 224)    \n",
    "                    target = target.flatten()\n",
    "                    target = target.type(torch.LongTensor)\n",
    "            \n",
    "                    output = model(images.cuda(gpu, non_blocking=True))\n",
    "                    acc1, acc5 = accuracy(\n",
    "                        output, target.cuda(gpu, non_blocking=True), topk=(1, 5)\n",
    "                    )\n",
    "                    top1.update(acc1[0].item(), images.size(0))\n",
    "                    top5.update(acc5[0].item(), images.size(0))\n",
    "            best_acc.top1 = max(best_acc.top1, top1.avg)\n",
    "            best_acc.top5 = max(best_acc.top5, top5.avg)\n",
    "            print('best_acc.top1', best_acc.top1)\n",
    "            print('best_acc.top5', best_acc.top5)\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "    ids_labels_val = pd.read_csv('../../ids_and_labels_val.csv',delimiter=',',header=1)\n",
    "    ids_labels_val = np.array(ids_labels_val)\n",
    "    print(ids_labels_val.shape)\n",
    "    val_classes = np.arange(len(np.unique(ids_labels_val[:,3].astype('int'))))\n",
    "    classes = np.unique(val_classes)\n",
    "    print(len(classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for i in range(len(classes))]\n",
    "        n_class_samples = [0 for i in range(len(classes))]\n",
    "        for images, labels in val_loader:\n",
    "        \n",
    "            images = images.view(-1, 3, 224, 224)     \n",
    "            labels = labels.flatten()\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "        \n",
    "            images = images.to(gpu)  \n",
    "            #print(labels.shape)\n",
    "            labels = labels.to(gpu)\n",
    "            outputs = model(images)\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if (label == pred):\n",
    "                    n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "        for i in range(len(classes)):\n",
    "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of {classes[i]}: {acc} %')\n",
    "        \n",
    "\n",
    "def handle_sigusr1(signum, frame):\n",
    "    os.system(f'scontrol requeue {os.getenv(\"SLURM_JOB_ID\")}')\n",
    "    exit()\n",
    "\n",
    "\n",
    "def handle_sigterm(signum, frame):\n",
    "    pass\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8526fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "Epoch [6/20], Step [30/5], Loss: 88.6476\n",
      "training_loss 3976.351356681188 54\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "Epoch [12/20], Step [60/5], Loss: 63.2206\n",
      "training_loss 137.72008034388224 114\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "Epoch [18/20], Step [90/5], Loss: 3.3087\n",
      "training_loss 29.716626000404357 174\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "best_acc.top1 1.9662921348314606\n",
      "best_acc.top5 9.55056179775281\n",
      "(354, 4)\n",
      "31\n",
      "Accuracy of the network: 1.9662921348314606 %\n",
      "Accuracy of 0: 100.0 %\n",
      "Accuracy of 1: 0.0 %\n",
      "Accuracy of 2: 0.0 %\n",
      "Accuracy of 3: 0.0 %\n",
      "Accuracy of 4: 0.0 %\n",
      "Accuracy of 5: 0.0 %\n",
      "Accuracy of 6: 0.0 %\n",
      "Accuracy of 7: 0.0 %\n",
      "Accuracy of 8: 0.0 %\n",
      "Accuracy of 9: 0.0 %\n",
      "Accuracy of 10: 0.0 %\n",
      "Accuracy of 11: 0.0 %\n",
      "Accuracy of 12: 0.0 %\n",
      "Accuracy of 13: 0.0 %\n",
      "Accuracy of 14: 0.0 %\n",
      "Accuracy of 15: 0.0 %\n",
      "Accuracy of 16: 0.0 %\n",
      "Accuracy of 17: 0.0 %\n",
      "Accuracy of 18: 0.0 %\n",
      "Accuracy of 19: 0.0 %\n",
      "Accuracy of 20: 0.0 %\n",
      "Accuracy of 21: 0.0 %\n",
      "Accuracy of 22: 0.0 %\n",
      "Accuracy of 23: 0.0 %\n",
      "Accuracy of 24: 0.0 %\n",
      "Accuracy of 25: 0.0 %\n",
      "Accuracy of 26: 0.0 %\n",
      "Accuracy of 27: 0.0 %\n",
      "Accuracy of 28: 0.0 %\n",
      "Accuracy of 29: 0.0 %\n",
      "Accuracy of 30: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "main_worker('cuda:2', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe9ae58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 4)\n",
      "31\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m n_class_correct \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classes))]\n\u001b[1;32m     12\u001b[0m n_class_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classes))]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_loader\u001b[49m:\n\u001b[1;32m     15\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)     \n\u001b[1;32m     16\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "ids_labels_val = pd.read_csv('../../ids_and_labels_val.csv',delimiter=',',header=1)\n",
    "ids_labels_val = np.array(ids_labels_val)\n",
    "print(ids_labels_val.shape)\n",
    "val_classes = np.arange(len(np.unique(ids_labels_val[:,3].astype('int'))))\n",
    "classes = np.unique(val_classes)\n",
    "print(len(classes))\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(len(classes))]\n",
    "    n_class_samples = [0 for i in range(len(classes))]\n",
    "    for images, labels in val_loader:\n",
    "        \n",
    "        images = images.view(-1, 3, 224, 224)     \n",
    "        labels = labels.flatten()\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        \n",
    "        images = images.to(device)  \n",
    "        #print(labels.shape)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d8d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

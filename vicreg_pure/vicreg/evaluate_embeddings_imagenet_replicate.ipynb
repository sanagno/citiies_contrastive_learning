{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34444cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import math\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb4d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import resnet\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import math\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True   #OTHERWISE TRUNCATED IMAGE FILE ERROR SOMEWHERE IN ENUMERATE(DATALOADER)!!!!\n",
    "\n",
    "import resnet\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from munkres import Munkres\n",
    "\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37120c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_preds(cluster_assignments, y_true, n_clusters):\n",
    "    '''\n",
    "    Computes the predicted labels, where label assignments now\n",
    "    correspond to the actual labels in y_true (as estimated by Munkres)\n",
    "\n",
    "    cluster_assignments:    array of labels, outputted by kmeans\n",
    "    y_true:                 true labels\n",
    "    n_clusters:             number of clusters in the dataset\n",
    "\n",
    "    returns:    a tuple containing the accuracy and confusion matrix,\n",
    "                in that order\n",
    "    '''\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_true, cluster_assignments, labels=None)\n",
    "    # compute accuracy based on optimal 1:1 assignment of clusters to labels\n",
    "    cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters)\n",
    "    indices = Munkres().compute(cost_matrix)\n",
    "    kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)\n",
    "    y_pred = kmeans_to_true_cluster_labels[cluster_assignments]\n",
    "    return y_pred, confusion_matrix \n",
    "\n",
    "def calculate_cost_matrix(C, n_clusters):\n",
    "    cost_matrix = np.zeros((n_clusters, n_clusters))\n",
    "\n",
    "    # cost_matrix[i,j] will be the cost of assigning cluster i to label j\n",
    "    for j in range(n_clusters):\n",
    "        s = np.sum(C[:,j]) # number of examples in cluster i\n",
    "        for i in range(n_clusters):\n",
    "            t = C[i,j]\n",
    "            cost_matrix[j,i] = s-t\n",
    "    return cost_matrix\n",
    "\n",
    "def get_cluster_labels_from_indices(indices):\n",
    "    n_clusters = len(indices)\n",
    "    clusterLabels = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        clusterLabels[i] = indices[i][1]\n",
    "    return clusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4290e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67e9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDP ATTEMPT\n",
    "\n",
    "def init_distributed():\n",
    "\n",
    "    # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n",
    "    dist_url = \"env://\" # default\n",
    "\n",
    "    # only works with torch.distributed.launch // torch.run\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    dist.init_process_group(\n",
    "            backend=\"nccl\",\n",
    "            init_method=dist_url,\n",
    "            world_size=world_size,\n",
    "            rank=rank)\n",
    "\n",
    "    # this will make all .cuda() calls work properly\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # synchronizes all the threads to reach this point before moving on\n",
    "    dist.barrier()\n",
    "    \n",
    "    print(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee23f147",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m val_transforms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     13\u001b[0m             [\n\u001b[1;32m     14\u001b[0m                 transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m             ]\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     22\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./ImageNet/ILSVRC/Data/CLS-LOC/train\u001b[39m\u001b[38;5;124m'\u001b[39m , train_transforms)\n\u001b[0;32m---> 23\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mDistributedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m                                                  \n\u001b[1;32m     24\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     25\u001b[0m                                             sampler\u001b[38;5;241m=\u001b[39mtrain_sampler, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#trainloader = DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle = False)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/distributed.py:66\u001b[0m, in \u001b[0;36mDistributedSampler.__init__\u001b[0;34m(self, dataset, num_replicas, rank, shuffle, seed, drop_last)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires distributed package to be available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     num_replicas \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:867\u001b[0m, in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:325\u001b[0m, in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03mHelper that gets a given group's world size.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:429\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#transforms = torchvision.transforms.Compose([torchvision.transforms.RandomResizedCrop(224), torchvision.transforms.ToTensor(),normalize])\n",
    "train_transforms = transforms.Compose(\n",
    "            [   transforms.Resize(256),\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder('./ImageNet/ILSVRC/Data/CLS-LOC/train' , train_transforms)\n",
    "train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)                                                  \n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            sampler=train_sampler, num_workers=10, pin_memory=True)\n",
    "#trainloader = DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "val_dataset = datasets.ImageFolder('./ImageNet/ILSVRC/Data/CLS-LOC/val' , val_transforms)\n",
    "val_sampler =DistributedSampler(dataset=val_dataset, shuffle=True)                                         \n",
    "valloader = torch.utils.data.DataLoader(val_datset, batch_size=batch_size,\n",
    "                                            shuffle=False, sampler=val_sampler, num_workers=10)\n",
    "#valloader = DataLoader(dataset=val_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4881302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ba3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12245794816, 12806062080)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.mem_get_info(torch.cuda.current_device()))\n",
    "device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e44f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_model_pretrained = torchvision.models.resnet50(pretrained=True)\n",
    "torch.save(supervised_model_pretrained.state_dict(), 'resnet50_imagenet_pretrained_supervised.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fba5a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone, embedding = resnet.__dict__['resnet50'](zero_init_residual=True)\n",
    "state_dict = torch.load('resnet50_imagenet_pretrained_supervised.pth', map_location=\"cpu\")\n",
    "if \"model\" in state_dict:\n",
    "    state_dict = state_dict[\"model\"]\n",
    "    state_dict = {key.replace(\"module.backbone.\", \"\"): value for (key, value) in state_dict.items()}\n",
    "backbone.load_state_dict(state_dict, strict=False)\n",
    "backbone.cuda()\n",
    "\n",
    "backbone = nn.SyncBatchNorm.convert_sync_batchnorm(backbone)\n",
    "\n",
    "local_rank = int(os.environ['LOCAL_RANK'])\n",
    "backbone = nn.parallel.DistributedDataParallel(backbone, device_ids=[local_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7052870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f1c147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m embeddings_unseen_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     10\u001b[0m         \n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#if i == 50000:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;66;03m#break\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28mprint\u001b[39m(i)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    248\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "labels_unseen_list = []\n",
    "embeddings_unseen_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        \n",
    "        #if i == 50000:\n",
    "            #break\n",
    "            \n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "            \n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_list.append(embedding)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095a51da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(valloader):\n",
    "        \n",
    "        #if i == 50000:\n",
    "            #break\n",
    "            \n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    \n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_unseen_list.append(embedding)\n",
    "        labels_unseen_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cacb7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.05462649e-01 6.24961317e-01 1.06552474e-01 ... 4.79363427e-02\n",
      "  2.51233071e-01 8.51763114e-02]\n",
      " [3.20454687e-01 7.22748399e-01 3.91381867e-02 ... 6.53433502e-02\n",
      "  3.63435537e-01 2.37048596e-01]\n",
      " [1.31849155e-01 4.85993207e-01 5.74591048e-02 ... 7.40184560e-02\n",
      "  4.56998110e-01 2.48788595e-01]\n",
      " ...\n",
      " [4.80764982e-04 2.68772274e-01 8.39343488e-01 ... 3.64980921e-02\n",
      "  1.55609727e-01 8.47105384e-01]\n",
      " [5.14939189e-01 2.45923138e+00 8.97173345e-01 ... 2.05933586e-01\n",
      "  1.99539721e-01 7.01217890e-01]\n",
      " [6.54005766e-01 1.52503222e-01 1.38447821e+00 ... 1.57990307e-01\n",
      "  2.03023940e-01 9.25557792e-01]]\n",
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852. 853.\n",
      " 854. 855. 856. 857. 858. 859. 860. 861. 862. 863. 864. 865. 866. 867.\n",
      " 868. 869. 870. 871. 872. 873. 874. 875. 876. 877. 878. 879. 880. 881.\n",
      " 882. 883. 884. 885. 886. 887. 888. 889. 890. 891. 892. 893. 894. 895.\n",
      " 896. 897. 898. 899. 900. 901. 902. 903. 904. 905. 906. 907. 908. 909.\n",
      " 910. 911. 912. 913. 914. 915. 916. 917. 918. 919. 920. 921. 922. 923.\n",
      " 924. 925. 926. 927. 928. 929. 930. 931. 932. 933. 934. 935. 936. 937.\n",
      " 938. 939. 940. 941. 942. 943. 944. 945. 946. 947. 948. 949. 950. 951.\n",
      " 952. 953. 954. 955. 956. 957. 958. 959. 960. 961. 962. 963. 964. 965.\n",
      " 966. 967. 968. 969. 970. 971. 972. 973. 974. 975. 976. 977. 978. 979.\n",
      " 980. 981. 982. 983. 984. 985. 986. 987. 988. 989. 990. 991. 992. 993.\n",
      " 994. 995. 996. 997. 998. 999.]\n",
      "[[0.46455982 0.31086484 0.20246169 ... 0.12877524 0.89597982 0.26228243]\n",
      " [0.08960301 0.93479466 0.55256152 ... 0.26709151 0.32134888 0.23873204]\n",
      " [0.02331889 0.81278461 0.14136121 ... 0.40902895 0.95786953 0.44216919]\n",
      " ...\n",
      " [0.15229265 0.31799147 0.48369917 ... 0.13289736 0.34216553 0.24501473]\n",
      " [0.24620102 0.3556084  0.47176239 ... 0.15194669 0.         0.0600398 ]\n",
      " [0.29789808 1.23763728 1.1165961  ... 0.24421902 0.28691047 0.40186214]]\n",
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852. 853.\n",
      " 854. 855. 856. 857. 858. 859. 860. 861. 862. 863. 864. 865. 866. 867.\n",
      " 868. 869. 870. 871. 872. 873. 874. 875. 876. 877. 878. 879. 880. 881.\n",
      " 882. 883. 884. 885. 886. 887. 888. 889. 890. 891. 892. 893. 894. 895.\n",
      " 896. 897. 898. 899. 900. 901. 902. 903. 904. 905. 906. 907. 908. 909.\n",
      " 910. 911. 912. 913. 914. 915. 916. 917. 918. 919. 920. 921. 922. 923.\n",
      " 924. 925. 926. 927. 928. 929. 930. 931. 932. 933. 934. 935. 936. 937.\n",
      " 938. 939. 940. 941. 942. 943. 944. 945. 946. 947. 948. 949. 950. 951.\n",
      " 952. 953. 954. 955. 956. 957. 958. 959. 960. 961. 962. 963. 964. 965.\n",
      " 966. 967. 968. 969. 970. 971. 972. 973. 974. 975. 976. 977. 978. 979.\n",
      " 980. 981. 982. 983. 984. 985. 986. 987. 988. 989. 990. 991. 992. 993.\n",
      " 994. 995. 996. 997. 998. 999.]\n"
     ]
    }
   ],
   "source": [
    "#USE THIS IF BATCHSIZE>1\n",
    "embeddings_seen_arr = np.zeros((len(trainloader)*batch_size-3, 2048))\n",
    "counter = 0\n",
    "for embedding in embeddings_list:\n",
    "    if len(embedding) == 10:\n",
    "        #print(len(embedding))\n",
    "        embeddings_seen_arr[counter:counter+batch_size,:] = embedding.cpu().detach().numpy()\n",
    "        counter += batch_size\n",
    "    else:\n",
    "        #print(len(embedding))\n",
    "        embeddings_seen_arr[counter:counter+7,:] = embedding.cpu().detach().numpy()\n",
    "        counter += 7\n",
    "\n",
    "print(embeddings_seen_arr)\n",
    "labels_seen_arr = np.zeros(len(trainloader)*batch_size-3)\n",
    "counter = 0\n",
    "for i in labels_list:\n",
    "    if len(i) == 10:\n",
    "        labels_seen_arr[counter:counter+batch_size] = i.detach().numpy()\n",
    "    else:\n",
    "        labels_seen_arr[counter:counter+7] = i.detach().numpy()\n",
    "    counter += batch_size\n",
    "print(np.unique(labels_seen_arr))\n",
    "\n",
    "embeddings_unseen_arr = np.zeros((len(valloader)*batch_size, 2048))\n",
    "counter = 0\n",
    "for embedding in embeddings_unseen_list:\n",
    "    embeddings_unseen_arr[counter:counter+batch_size,:] = embedding.cpu().detach().numpy()\n",
    "    counter += batch_size\n",
    "\n",
    "print(embeddings_unseen_arr)\n",
    "labels_unseen_arr = np.zeros(len(valloader)*batch_size)\n",
    "counter = 0\n",
    "for i in labels_unseen_list:\n",
    "    labels_unseen_arr[counter:counter+batch_size] = i.detach().numpy()\n",
    "    counter += batch_size\n",
    "print(np.unique(labels_unseen_arr))\n",
    "\n",
    "embeddings_seen_arr = np.delete(embeddings_seen_arr,0,axis=0)\n",
    "labels_seen_arr = np.delete(labels_seen_arr,0)\n",
    "embeddings_unseen_arr = np.delete(embeddings_unseen_arr,0,axis=0)\n",
    "labels_unseen_arr = np.delete(labels_unseen_arr,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d3c59ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14875932 0.40740678 0.24900784 ... 0.42377618 0.62659913 0.26762429]\n",
      " [0.15100792 0.57644802 0.06908274 ... 0.32252085 1.0731976  0.4173148 ]\n",
      " [0.14548095 0.86708254 0.20886353 ... 0.20139341 1.05986702 0.19876178]\n",
      " ...\n",
      " [0.08778701 0.33687139 1.03078735 ... 0.24490468 0.23621872 0.15081446]\n",
      " [0.24522467 0.36625695 0.32675669 ... 0.25291982 0.05003487 0.1373997 ]\n",
      " [0.26443723 0.60494483 1.97403979 ... 0.34973338 0.31702667 0.34852833]]\n",
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852. 853.\n",
      " 854. 855. 856. 857. 858. 859. 860. 861. 862. 863. 864. 865. 866. 867.\n",
      " 868. 869. 870. 871. 872. 873. 874. 875. 876. 877. 878. 879. 880. 881.\n",
      " 882. 883. 884. 885. 886. 887. 888. 889. 890. 891. 892. 893. 894. 895.\n",
      " 896. 897. 898. 899. 900. 901. 902. 903. 904. 905. 906. 907. 908. 909.\n",
      " 910. 911. 912. 913. 914. 915. 916. 917. 918. 919. 920. 921. 922. 923.\n",
      " 924. 925. 926. 927. 928. 929. 930. 931. 932. 933. 934. 935. 936. 937.\n",
      " 938. 939. 940. 941. 942. 943. 944. 945. 946. 947. 948. 949. 950. 951.\n",
      " 952. 953. 954. 955. 956. 957. 958. 959. 960. 961. 962. 963. 964. 965.\n",
      " 966. 967. 968. 969. 970. 971. 972. 973. 974. 975. 976. 977. 978. 979.\n",
      " 980. 981. 982. 983. 984. 985. 986. 987. 988. 989. 990. 991. 992. 993.\n",
      " 994. 995. 996. 997. 998. 999.]\n",
      "[[0.44182488 0.15303479 0.12645964 ... 0.28049049 0.88026887 0.53171641]\n",
      " [0.03740757 0.34985149 0.38955975 ... 0.06550678 0.33117694 0.22765315]\n",
      " [0.30357459 0.53781545 0.2005505  ... 0.07402524 0.34682354 0.20791489]\n",
      " ...\n",
      " [0.40609333 0.54102761 0.43587345 ... 0.3030937  0.01270484 1.39897931]\n",
      " [0.07913039 0.14274192 0.75967175 ... 0.34730536 0.45179495 0.74191493]\n",
      " [0.32506564 0.38684285 2.04378343 ... 0.13303126 0.36846662 0.75627124]]\n",
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852. 853.\n",
      " 854. 855. 856. 857. 858. 859. 860. 861. 862. 863. 864. 865. 866. 867.\n",
      " 868. 869. 870. 871. 872. 873. 874. 875. 876. 877. 878. 879. 880. 881.\n",
      " 882. 883. 884. 885. 886. 887. 888. 889. 890. 891. 892. 893. 894. 895.\n",
      " 896. 897. 898. 899. 900. 901. 902. 903. 904. 905. 906. 907. 908. 909.\n",
      " 910. 911. 912. 913. 914. 915. 916. 917. 918. 919. 920. 921. 922. 923.\n",
      " 924. 925. 926. 927. 928. 929. 930. 931. 932. 933. 934. 935. 936. 937.\n",
      " 938. 939. 940. 941. 942. 943. 944. 945. 946. 947. 948. 949. 950. 951.\n",
      " 952. 953. 954. 955. 956. 957. 958. 959. 960. 961. 962. 963. 964. 965.\n",
      " 966. 967. 968. 969. 970. 971. 972. 973. 974. 975. 976. 977. 978. 979.\n",
      " 980. 981. 982. 983. 984. 985. 986. 987. 988. 989. 990. 991. 992. 993.\n",
      " 994. 995. 996. 997. 998. 999.]\n"
     ]
    }
   ],
   "source": [
    "#embeddings_seen_arr = np.zeros((int(3*len(dataloader)/4), 2048))\n",
    "#counter = 0\n",
    "#for embedding in embeddings_list:\n",
    "#    embeddings_seen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "##    counter += 1\n",
    "\n",
    "#print(embeddings_seen_arr)\n",
    "#labels_seen_arr = np.zeros(int(3*len(dataloader)/4))\n",
    "#counter = 0\n",
    "#for i in labels_list:\n",
    "#    labels_seen_arr[counter] = i.detach().numpy()\n",
    "#    counter += 1\n",
    "#print(np.unique(labels_seen_arr))\n",
    "\n",
    "#embeddings_unseen_arr = np.zeros((int(len(dataloader)/4), 2048))\n",
    "#counter = 0\n",
    "#for embedding in embeddings_unseen_list:\n",
    "#    embeddings_unseen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "#    counter += 1\n",
    "\n",
    "##print(embeddings_unseen_arr)\n",
    "#labels_unseen_arr = np.zeros(int(len(dataloader)/4))\n",
    "#counter = 0\n",
    "#for i in labels_unseen_list:\n",
    "#    labels_unseen_arr[counter] = i.detach().numpy()\n",
    "#    counter += 1\n",
    "#print(np.unique(labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de6be9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTER KNN IMPLEMENTATION!!!!\n",
    "\n",
    "class FaissKNeighbors:\n",
    "    def __init__(self, k):\n",
    "        self.index = None\n",
    "        self.y = None\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.index = faiss.IndexFlatL2(X.shape[1])\n",
    "        self.index.add(X.astype(np.float32))\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n",
    "        votes = self.y[indices]\n",
    "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes.astype(np.int64)])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e84d12c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy20: 0.7326746534930698\n",
      "Accuracy200: 0.7158143162863257\n"
     ]
    }
   ],
   "source": [
    "n_neighbours = [20, 200]\n",
    "for k in n_neighbours:\n",
    "    \n",
    "    #knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn = FaissKNeighbors(k = k)\n",
    "    knn.fit(embeddings_seen_arr, labels_seen_arr)\n",
    "    labels_predicted = knn.predict(embeddings_unseen_arr)\n",
    "\n",
    "    print('Accuracy' + str(k) + ':', np.sum(labels_unseen_arr == labels_predicted)/len(labels_unseen_arr))\n",
    "    #print('Accuracy:', knn.score(embeddings_unseen_arr, labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4645602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#JUST TRY DELETE LATER\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#embeddings_unseen_arr_trial = embeddings_unseen_arr[0:180,:]\n",
    "#labels_unseen_arr_trial = labels_unseen_arr[0:180]\n",
    "\n",
    "#pca = PCA(n_components=2)\n",
    "#pca.fit(embeddings_unseen_arr)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "\n",
    "#embeddings_reduced = pca.transform(embeddings_unseen_arr)\n",
    "#print(embeddings_reduced)\n",
    "\n",
    "#u_labels = np.unique(labels_unseen_arr_trial)\n",
    "#print(u_labels)\n",
    "\n",
    "#embeddings_reduced = TSNE(n_components=2, learning_rate='auto',init='random', perplexity = 10.0).fit_transform(embeddings_unseen_arr_trial)\n",
    "\n",
    "#for i in u_labels:\n",
    "    \n",
    "#    plt.scatter(embeddings_reduced[labels_unseen_arr_trial == i , 0] , embeddings_reduced[labels_unseen_arr_trial == i , 1] , label = i)\n",
    "\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9146f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53104\n",
      "Accuracy: 0.5348\n",
      "Accuracy: 0.56032\n",
      "Accuracy: 0.57928\n",
      "Accuracy: 0.58632\n",
      "Accuracy: 0.58416\n",
      "Accuracy: 0.58\n",
      "Accuracy: 0.57856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neighbours = [1, 3, 5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for k in n_neighbours:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(embeddings_seen_arr, labels_seen_arr)\n",
    "    labels_predicted = knn.predict(embeddings_unseen_arr)\n",
    "\n",
    "    #print('Accuracy' + str(k) + ':', np.sum(labels_unseen_arr == labels_predicted)/len(labels_unseen_arr))\n",
    "    print('Accuracy:', knn.score(embeddings_unseen_arr, labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd153ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[328 199 199 ... 239 239 123]\n",
      "0.9985074901992159\n",
      "0.474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=1000, random_state=0).fit(embeddings_seen_arr)\n",
    "labels_predicted = kmeans.predict(embeddings_unseen_arr)\n",
    "print(labels_predicted)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.rand_score(labels_unseen_arr, labels_predicted))\n",
    "\n",
    "truth = labels_unseen_arr\n",
    "pred=labels_predicted\n",
    "print(np.sum( get_y_preds(pred, truth, 1000)[0] == truth )/len(truth) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e2fcb",
   "metadata": {},
   "source": [
    "# VICREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ec780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /local/home/bsoyuer/.cache/torch/hub/facebookresearch_vicreg_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRY LIKE THIS\n",
    "#backbone = torch.hub.load('facebookresearch/vicreg:main', 'resnet50')\n",
    "#backbone.to(device)\n",
    "#backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "vicreg_model_pretrained = torch.hub.load('facebookresearch/vicreg:main', 'resnet50')\n",
    "torch.save(vicreg_model_pretrained.state_dict(), 'resnet50_imagenet_pretrained_vicreg.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de8edc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone, embedding = resnet.__dict__['resnet50'](zero_init_residual=True)\n",
    "state_dict = torch.load('resnet50_imagenet_pretrained_vicreg.pth', map_location=\"cpu\")\n",
    "if \"model\" in state_dict:\n",
    "    state_dict = state_dict[\"model\"]\n",
    "    state_dict = {key.replace(\"module.backbone.\", \"\"): value for (key, value) in state_dict.items()}\n",
    "backbone.load_state_dict(state_dict, strict=False)\n",
    "backbone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e55c550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (last_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bb401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "85000\n",
      "85100\n",
      "85200\n",
      "85300\n",
      "85400\n",
      "85500\n",
      "85600\n",
      "85700\n"
     ]
    }
   ],
   "source": [
    "labels_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "labels_unseen_list = []\n",
    "embeddings_unseen_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        \n",
    "        #if i == 50000:\n",
    "            #break\n",
    "            \n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        \n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_list.append(embedding)\n",
    "        labels_list.append(labels)\n",
    "        \n",
    "        #torch.cuda.empty_cache()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(valloader):\n",
    "        \n",
    "        #if i == 50000:\n",
    "            #break\n",
    "            \n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    \n",
    "        embedding = backbone(inputs.to(device))\n",
    "        embeddings_unseen_list.append(embedding)\n",
    "        labels_unseen_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE THIS IF BATCHSIZE>1\n",
    "embeddings_seen_arr = np.zeros((len(trainloader)*batch_size-3, 2048))\n",
    "counter = 0\n",
    "for embedding in embeddings_list:\n",
    "    if len(embedding) == 10:\n",
    "        #print(len(embedding))\n",
    "        embeddings_seen_arr[counter:counter+batch_size,:] = embedding.cpu().detach().numpy()\n",
    "        counter += batch_size\n",
    "    else:\n",
    "        #print(len(embedding))\n",
    "        embeddings_seen_arr[counter:counter+7,:] = embedding.cpu().detach().numpy()\n",
    "        counter += 7\n",
    "\n",
    "print(embeddings_seen_arr)\n",
    "labels_seen_arr = np.zeros(len(trainloader)*batch_size-3)\n",
    "counter = 0\n",
    "for i in labels_list:\n",
    "    if len(i) == 10:\n",
    "        labels_seen_arr[counter:counter+batch_size] = i.detach().numpy()\n",
    "    else:\n",
    "        labels_seen_arr[counter:counter+7] = i.detach().numpy()\n",
    "    counter += batch_size\n",
    "print(np.unique(labels_seen_arr))\n",
    "\n",
    "embeddings_unseen_arr = np.zeros((len(valloader)*batch_size, 2048))\n",
    "counter = 0\n",
    "for embedding in embeddings_unseen_list:\n",
    "    embeddings_unseen_arr[counter:counter+batch_size,:] = embedding.cpu().detach().numpy()\n",
    "    counter += batch_size\n",
    "\n",
    "print(embeddings_unseen_arr)\n",
    "labels_unseen_arr = np.zeros(len(valloader)*batch_size)\n",
    "counter = 0\n",
    "for i in labels_unseen_list:\n",
    "    labels_unseen_arr[counter:counter+batch_size] = i.detach().numpy()\n",
    "    counter += batch_size\n",
    "print(np.unique(labels_unseen_arr))\n",
    "\n",
    "embeddings_seen_arr = np.delete(embeddings_seen_arr,0,axis=0)\n",
    "labels_seen_arr = np.delete(labels_seen_arr,0)\n",
    "embeddings_unseen_arr = np.delete(embeddings_unseen_arr,0,axis=0)\n",
    "labels_unseen_arr = np.delete(labels_unseen_arr,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d20bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(embeddings_seen_arr.shape[0]):\n",
    "    #if len(np.unique(embeddings_seen_arr[i,:]))==1:\n",
    "        #print('yes')\n",
    "for i in range(embeddings_unseen_arr.shape[0]):\n",
    "    if len(np.unique(embeddings_unseen_arr[i,:]))==1:\n",
    "        print('yes_unseen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c94851cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (7,2048) into shape (10,2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings_list:\n\u001b[0;32m----> 4\u001b[0m     embeddings_seen_arr[counter:counter\u001b[38;5;241m+\u001b[39mbatch_size,:] \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings_seen_arr)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (7,2048) into shape (10,2048)"
     ]
    }
   ],
   "source": [
    "#embeddings_seen_arr = np.zeros((len(trainloader), 2048))\n",
    "#counter = 0\n",
    "#for embedding in embeddings_list:\n",
    "#    embeddings_seen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "#    counter += 1\n",
    "\n",
    "#print(embeddings_seen_arr)\n",
    "#labels_seen_arr = np.zeros(len(trainloader))\n",
    "#counter = 0\n",
    "#for i in labels_list:\n",
    "#    labels_seen_arr[counter] = i.detach().numpy()\n",
    "#    counter += 1\n",
    "#print(np.unique(labels_seen_arr))\n",
    "\n",
    "#embeddings_unseen_arr = np.zeros((len(valloader), 2048))\n",
    "#counter = 0\n",
    "#for embedding in embeddings_unseen_list:\n",
    "#    embeddings_unseen_arr[counter,:] = embedding.cpu().detach().numpy()\n",
    "#    counter += 1\n",
    "\n",
    "#print(embeddings_unseen_arr)\n",
    "#labels_unseen_arr = np.zeros(len(valloader))\n",
    "#counter = 0\n",
    "#for i in labels_unseen_list:\n",
    "#    labels_unseen_arr[counter] = i.detach().numpy()\n",
    "#    counter += 1\n",
    "#print(np.unique(labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec0d7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTER KNN IMPLEMENTATION!!!!\n",
    "\n",
    "class FaissKNeighbors:\n",
    "    def __init__(self, k):\n",
    "        self.index = None\n",
    "        self.y = None\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.index = faiss.IndexFlatL2(X.shape[1])\n",
    "        self.index.add(X.astype(np.float32))\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n",
    "        votes = self.y[indices]\n",
    "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes.astype(np.int64)])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7a74e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy20: 0.6193723874477489\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m knn \u001b[38;5;241m=\u001b[39m FaissKNeighbors(k \u001b[38;5;241m=\u001b[39m k)\n\u001b[1;32m      9\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(embeddings_seen_arr, labels_seen_arr)\n\u001b[0;32m---> 10\u001b[0m labels_predicted \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_unseen_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(labels_unseen_arr \u001b[38;5;241m==\u001b[39m labels_predicted)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels_unseen_arr))\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mFaissKNeighbors.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 15\u001b[0m     distances, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     votes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[indices]\n\u001b[1;32m     17\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mbincount(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m votes\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/faiss/__init__.py:322\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, D, I)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m I\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (n, k)\n\u001b[0;32m--> 322\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswig_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswig_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswig_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m D, I\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py:2146\u001b[0m, in \u001b[0;36mIndexFlat.search\u001b[0;34m(self, n, x, k, distances, labels)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, x, k, distances, labels):\n\u001b[0;32m-> 2146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_swigfaiss_avx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexFlat_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#n_neighbours = [1, 3, 5, 10, 20, 30, 40, 50]\n",
    "n_neighbours = [20, 200]\n",
    "for k in n_neighbours:\n",
    "    \n",
    "    #knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn = FaissKNeighbors(k = k)\n",
    "    knn.fit(embeddings_seen_arr, labels_seen_arr)\n",
    "    labels_predicted = knn.predict(embeddings_unseen_arr)\n",
    "\n",
    "    print('Accuracy' + str(k) + ':', np.sum(labels_unseen_arr == labels_predicted)/len(labels_unseen_arr))\n",
    "    #print('Accuracy:', knn.score(embeddings_unseen_arr, labels_unseen_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d32e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('embeddings_imagenet_seen_vicreg.txt', embeddings_seen_arr)\n",
    "#np.savetxt('labels_imagenet_seen_vicreg.txt', labels_seen_arr, fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67aa0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#JUST TRY DELETE LATER\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#embeddings_unseen_arr_trial = embeddings_unseen_arr[0:180,:]\n",
    "#labels_unseen_arr_trial = labels_unseen_arr[0:180]\n",
    "\n",
    "#pca = PCA(n_components=2)\n",
    "#pca.fit(embeddings_unseen_arr)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "\n",
    "#embeddings_reduced = pca.transform(embeddings_unseen_arr)\n",
    "#print(embeddings_reduced)\n",
    "\n",
    "#u_labels = np.unique(labels_unseen_arr_trial)\n",
    "#print(u_labels)\n",
    "\n",
    "#embeddings_reduced = TSNE(n_components=2, learning_rate='auto',init='random', perplexity = 10.0).fit_transform(embeddings_unseen_arr_trial)\n",
    "\n",
    "#for i in u_labels:\n",
    "    \n",
    "#    plt.scatter(embeddings_reduced[labels_unseen_arr_trial == i , 0] , embeddings_reduced[labels_unseen_arr_trial == i , 1] , label = i)\n",
    "\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382c4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
